# ğŸ“š RAG con LangChain, ChromaDB y Ollama

Este proyecto implementa un sistema de **RAG (Retrieval-Augmented Generation)** local. El sistema indexa los documentos en una base de datos vectorial y utiliza un LLM (Llama 3.2) para responder preguntas basÃ¡ndose estrictamente en el contenido de esos archivos.

## ğŸš€ CaracterÃ­sticas

* **Procesamiento de Documentos:** Carga y divide archivos PDF de la carpeta Informes`.
* **Base de Datos Vectorial:** Utiliza **ChromaDB** para almacenar los embeddings de forma persistente.
* **Embeddings MultilingÃ¼es:** Usa `paraphrase-multilingual-mpnet-base-v2` para una bÃºsqueda semÃ¡ntica precisa en espaÃ±ol e inglÃ©s.
* **Inferencia Local:** Utiliza **Ollama** con el modelo `llama3.2`, garantizando privacidad y ejecuciÃ³n offline.

## ğŸ“‹ Requisitos Previos

1.  **Python 3.10+** instalado.
2.  **Ollama** instalado y ejecutÃ¡ndose.
    * DescÃ¡rgalo en [ollama.com](https://ollama.com).
    * Descarga el modelo necesario ejecutando en tu terminal:
        ```bash
        ollama pull llama3.2
        ```

## ğŸ› ï¸ InstalaciÃ³n

1.  Clona este repositorio o descarga los archivos.
2.  Crea un entorno virtual (opcional pero recomendado):
    ```bash
    python -m venv venv
    source venv/bin/activate  # En Windows: venv\Scripts\activate
    ```
3.  Instala las dependencias necesarias:
    ```bash
    pip install -r requirements.txt
    ```

## ğŸ“‚ Estructura del Proyecto

Antes de ejecutar, asegÃºrate de tener la siguiente estructura:

```text
.
â”œâ”€â”€ create_database.py    # Script para generar/actualizar la base de datos
â”œâ”€â”€ query_database.py     # Script para realizar consultas
â”œâ”€â”€ Informes/             # Carpeta con los documentos financieros
â””â”€â”€ chroma_db/            # Se generarÃ¡ automÃ¡ticamente aquÃ­ la base de datos
